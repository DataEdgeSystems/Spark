package com.spark.mangodb

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.hadoop.conf.Configuration
import org.bson.BSONObject
import org.bson.BasicBSONObject
import com.mongodb.hadoop.BSONFileOutputFormat
import com.mongodb.hadoop.MongoOutputFormat
import com.mongodb.hadoop.io.MongoUpdateWritable
import com.mongodb.BasicDBObject

object Spark_MangoDB {

  def main(args: Array[String]) {

    val sc = new SparkContext("local[2]", "Spark-MangoDB-Intergation")
    val config = new Configuration()

    // mydb is database name , wordcount is the table/collection name .
    config.set("mongo.input.uri", "mongodb://127.0.0.1:27017/mydb.wordcount")

    //reading the data from the wordcount table/collection
    val mongoRDD = sc.newAPIHadoopRDD(config, classOf[com.mongodb.hadoop.MongoInputFormat], classOf[Object], classOf[BSONObject])

    //printing the records in the table.
    mongoRDD.foreach(f => println(f._2))

    // Input contains tuples of (ObjectId, BSONObject)
    //performing the word count logic on the "text" column in the table.
    val countsRDD = mongoRDD.flatMap(arg => {
      var str = arg._2.get("text").toString
      str = str.toLowerCase().replaceAll("[.,!?\n]", " ")
      str.split(" ")
    })
      .map(word => (word, 1))
      .reduceByKey((a, b) => a + b)

    countsRDD.collect().foreach(f => println(f))

    // Output contains tuples of (null, BSONObject) - ObjectId will be generated by Mongo driver if null
    //forming a BSON object to insert the output in the table.
    val saveRDD = countsRDD.map((tuple) => {
      var bson = new BasicBSONObject()
      bson.put("word", tuple._1)
      bson.put("count", tuple._2.toString())
      (null, bson)
    })

    saveRDD.collect().foreach(f => println(f._2))

    //Saving the word count results to a new table called wcoutput.
    config.set("mongo.output.uri", "mongodb://127.0.0.1:27017/mydb.wcoutput")
    saveRDD.saveAsNewAPIHadoopFile(
      "file:///this-is-completely-unused",
      classOf[Object], classOf[BSONObject], classOf[MongoOutputFormat[Object, BSONObject]],
      config)

    // We can also save this back to a BSON file.
    saveRDD.saveAsNewAPIHadoopFile(
      "E:/Software/Spark/data/bson",
      classOf[Object], classOf[BSONObject], classOf[BSONFileOutputFormat[Object, BSONObject]])

  }

  //http://www.liquidweb.com/kb/how-to-install-mongodb-on-centos-7/
  
  //inserting the data in the wordcount table.
  /*
   	use mydb
		db.wordcount.insert({"text":"intergating spark with mangaodb"})
		db.wordcount.insert({"text":"intergating spark with Hbase"})
		db.wordcount.insert({"text":"intergating spark with Hive"})
		db.wordcount.insert({"text":"intergating spark with HDFS"})
		db.wordcount.insert({"text":"intergating spark with Cassandra"})
		db.wordcount.find().pretty()

		db.wcoutput.find().pretty()
	*/
}